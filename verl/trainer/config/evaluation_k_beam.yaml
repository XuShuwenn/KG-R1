# Evaluation config for KG search with Pass@K metrics
# Based on ppo_trainer.yaml but adapted for evaluation

# dataset config
data:
  # Tokenizer class or path. If null, it will be inferred from the model.
  tokenizer: null

  # Whether to use shared memory for data loading.
  use_shm: False

  # Training set parquet - for evaluation, this will be the test set
  train_files: ~/data/kg/test.parquet

  # Validation parquet - same as train for evaluation
  val_files: ~/data/kg/test.parquet

  # The field in the dataset where the prompt is located. Default is 'prompt'.
  prompt_key: prompt

  # The field used to select the reward function (if using different ones per example).
  reward_fn_key: data_source

  # Maximum prompt length. All prompts will be left-padded to this length.
  max_prompt_length: 3500

  # Maximum response length. Rollout in RL algorithms (e.g. PPO) generates up to this length.
  max_response_length: 256

  # Maximum observation length for search-augmented generation
  max_obs_length: 512

  # Batch size sampled for one training iteration of different RL algorithms.
  train_batch_size: 64

  # Batch size used during validation. Can be null.
  val_batch_size: 128

  # Whether to return the original input_ids without adding chat template.
  return_raw_input_ids: False

  # Whether to return the original chat (prompt) without applying chat template.
  return_raw_chat: False

  # Whether to return the full prompt with chat template.
  return_full_prompt: False

  # Whether to shuffle the data in the dataloader.
  shuffle: False

  # Whether to shuffle the validation set.
  validation_shuffle: False

  # Whether to filter overlong prompts.
  filter_overlong_prompts: False

  # Number of workers for filtering overlong prompts.
  filter_overlong_prompts_workers: 1

  # Truncate the input_ids or prompt if they exceed max_prompt_length.
  truncation: error

  # The field in the multi-modal dataset where the image is located.
  image_key: images

  # The field in the multi-modal dataset where the video is located.
  video_key: videos

  # If the remote tokenizer has a Python file, this flag determines whether to allow using it.
  trust_remote_code: True

  # Prompt augmentation for KG tasks
  prompt_augmentation:
    enable: True
    guideline_level: detailed_flat
    hint_steps: 500

# config for actor, rollout and reference model
actor_rollout_ref:
  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: True

  # common configs for the model
  model:
    # Huggingface model path. This will be overridden by checkpoint path
    path: ~/models/qwen2.5-3b-instruct

    # Whether to use shared memory (SHM) for accelerating the loading of model weights
    use_shm: False

    # Additional Python packages to register huggingface models/tokenizers.
    external_lib: null
    
    # Used to override model's original configurations, mainly dropout
    override_config: {}
    
    # Enable gradient checkpointing for actor
    enable_gradient_checkpointing: False
    
    # Enable activation offloading for actor
    enable_activation_offload: False
    
    # Whether to remove padding tokens in inputs during training
    use_remove_padding: True
    
    # Set to positive value to enable LoRA (e.g., 32)
    lora_rank: 0
    
    # LoRA scaling factor
    lora_alpha: 16
    
    # Target modules to apply LoRA
    target_modules: all-linear
    
    # Whether to use Liger for linear layer fusion
    use_liger: False
    
    # Whether to use custom fused kernels (e.g., FlashAttention, fused MLP)
    use_fused_kernels: False
    
    # Whether to enable loading a remote code model
    trust_remote_code: True

    attn_implementation: "sdpa"

  # configs for the actor
  actor:
    # fsdp, fsdp2 or megatron. fsdp backend used here.
    strategy: fsdp
    
    # Split each sample into sub-batches of this size for PPO
    ppo_mini_batch_size: 64
    
    # Global micro batch size
    ppo_micro_batch_size: null
    
    # Local per-GPU micro batch size
    ppo_micro_batch_size_per_gpu: 8
    
    # Whether to automatically adjust batch size at runtime
    use_dynamic_bsz: False
    
    # Max tokens per GPU in one PPO batch
    ppo_max_token_len_per_gpu: 16384
    
    # Gradient clipping for actor updates
    grad_clip: 1.0
    
    # PPO clip ratio
    clip_ratio: 0.2
    
    # Lower bound for asymmetric clipping
    clip_ratio_low: 0.2
    
    # Upper bound for asymmetric clipping
    clip_ratio_high: 0.2
    
    # Constant C in Dual-clip PPO
    clip_ratio_c: 3.0
    
    # Loss aggregation mode
    loss_agg_mode: token-mean
    
    # Entropy regularization coefficient in PPO loss
    entropy_coeff: 0
    
    # Whether to use KL loss instead of KL reward penalty. True for GRPO
    use_kl_loss: False
    
    # Whether to enable state masking for multi-turn conversations
    state_masking: False
    
    # Whether to use torch.compile()
    use_torch_compile: True
    
    # KL loss coefficient when use_kl_loss is enabled. For GRPO
    kl_loss_coef: 0.001
    
    # Type of KL divergence loss
    kl_loss_type: low_var_kl
    
    # Number of PPO epochs per batch
    ppo_epochs: 1
    
    # Shuffle training data across PPO epochs
    shuffle: False
    
    # Sequence parallelism size for Ulysses-style model parallelism
    ulysses_sequence_parallel_size: 1
    
    # checkpoint configs
    checkpoint:
      contents: ['model', 'optimizer', 'extra']

    # optimizer configs
    optim:
      lr: 1e-6
      lr_warmup_steps: -1
      lr_warmup_steps_ratio: 0.0
      min_lr_ratio: 0.0
      num_cycles: 0.5
      warmup_style: constant
      total_training_steps: -1
      weight_decay: 0.01

    # configs for FSDP
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: False
      optimizer_offload: False
      offload_policy: False
      reshard_after_forward: True
      fsdp_size: -1

  # Reference model config
  ref:
    strategy: fsdp
    fsdp_config:
      param_offload: True
      reshard_after_forward: True
      wrap_policy:
        min_num_params: 0
    use_torch_compile: True
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: null
    log_prob_use_dynamic_bsz: False
    log_prob_max_token_len_per_gpu: 130000
    ulysses_sequence_parallel_size: 1
    
  # Rollout model config
  rollout:
    name: vllm
    mode: sync
    temperature: 1.0
    top_k: -1
    top_p: 1
    use_fire_sampling: False
    prompt_length: 3500
    response_length: 256
    dtype: bfloat16
    gpu_memory_utilization: 0.8
    ignore_eos: False
    enforce_eager: False
    free_cache_engine: False
    load_format: dummy_dtensor
    layered_summon: False
    tensor_model_parallel_size: 1
    max_num_batched_tokens: 16384
    max_model_len: null
    max_num_seqs: 256
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 8
    log_prob_use_dynamic_bsz: False
    log_prob_max_token_len_per_gpu: 130000
    disable_log_stats: False
    enable_chunked_prefill: True
    do_sample: True
    n: 1

    # Extra inference engine arguments
    engine_kwargs:
      vllm:
        swap_space: null
      sglang:
        attention_backend: null

    # Sampling parameters used during validation
    val_kwargs:
      top_k: -1
      top_p: 1.0
      temperature: 0
      n: 1
      do_sample: False

    # Multi-turn interaction config
    multi_turn:
      enable: False
      max_turns: null
      tool_config_path: null
      format: chatml
      completion_callback: null

    # Search-augmented generation config
    search:
      enable: True
      search_url: "http://127.0.0.1:8001/retrieve"
      max_turns: 10
      topk: 3
      enable_during_training: False
      enable_during_validation: True
      timeout: 3

# configs for the critic
critic:
  rollout_n: 1
  strategy: fsdp
  
  optim:
    lr: 1e-5
    lr_warmup_steps_ratio: 0.
    min_lr_ratio: null
    warmup_style: constant
    total_training_steps: -1
    weight_decay: 0.01
    
  model:
    path: ~/models/qwen2.5-3b-instruct
    use_shm: False
    tokenizer_path: null
    override_config: {}
    external_lib: null
    enable_gradient_checkpointing: True
    enable_activation_offload: False
    use_remove_padding: False
    trust_remote_code: True
    
    fsdp_config:
      param_offload: False
      optimizer_offload: False
      offload_policy: False
      reshard_after_forward: True
      wrap_policy:
        min_num_params: 0
      fsdp_size: -1

    lora_rank: 0
    lora_alpha: 16
    target_modules: all-linear
    attn_implementation: "sdpa"

  ppo_mini_batch_size: 64
  ppo_micro_batch_size: null
  ppo_micro_batch_size_per_gpu: 8
  forward_micro_batch_size: null
  forward_micro_batch_size_per_gpu: 8
  use_dynamic_bsz: False
  ppo_max_token_len_per_gpu: 32768
  forward_max_token_len_per_gpu: 32768
  ulysses_sequence_parallel_size: 1
  ppo_epochs: 1
  shuffle: False
  grad_clip: 1.0
  cliprange_value: 0.5
  loss_agg_mode: token-mean
  
  checkpoint:
    contents: ['model', 'optimizer', 'extra']

# configs for the reward model
reward_model:
  enable: False
  strategy: fsdp
  
  model:
    input_tokenizer: null
    path: ~/models/reward-model
    use_shm: False
    external_lib: null
    use_remove_padding: False
    use_fused_kernels: False
    trust_remote_code: False
    
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: False
      reshard_after_forward: True
      fsdp_size: -1

  micro_batch_size: null
  micro_batch_size_per_gpu: 8
  max_length: null
  ulysses_sequence_parallel_size: 1
  use_dynamic_bsz: False
  forward_max_token_len_per_gpu: 32768
  
  # Reward Manager for KG evaluation
  reward_manager: kg_format_multiturn
  
  attn_implementation: "sdpa"
  launch_reward_fn_async: False
  
  sandbox_fusion:
    url: null
    max_concurrent: 64

  # Reward function arguments for KG evaluation
  reward_kwargs:
    turn_kg_query_validity: 0.5
    turn_is_answer_score: 0.5
    turn_format_score: 0.5
    global_exact_match: 0.5
    global_retrieval_quality: 0.5
    kg_server_error_penalty: 0
    kg_not_found_penalty: 0
    kg_format_error_penalty: 0
    kg_no_data_penalty: 0
    verbose: False
    debug_long_responses: True
    response_length_threshold: 3400
    debug_log_dir: evaluation_debug.log
    answer_score_mode: f1
    # OTC (Optimal Turn Count) scaling configuration - set to false by default
    otc_scaling: false
    # Maximum number of turns for OTC scaling calculation
    max_turns: 10

# custom reward function definition
custom_reward_function:
  path: null
  name: compute_score

# config for the algorithm
algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: True
  enable_multiturn_advantage: True
  use_kl_in_reward: False
  kl_penalty: kl
  
  kl_ctrl:
    type: fixed
    kl_coef: 0
    horizon: 10000
    target_kl: 0.1

  # KG token masking configuration
  kg_token_masking:
    enable: False
    reduction_factor: 0
    patterns: ["<kg-query>", "</kg-query>", "<search>", "</search>", "<think>", "</think>", "get_tail_relations", "get_head_relations", "get_tail_entities", "get_head_entities", "get_conditional_relations"]
    debug_logging: False

  use_pf_ppo: False
  pf_ppo:
    reweight_method: pow
    weight_pow: 2.0

  state_masking:
    start_state_marker: "<information>"
    end_state_marker: "</information>"

# config for the trainer
trainer:
  balance_batch: True
  total_epochs: 1
  total_training_steps: null
  project_name: KG-Evaluation
  experiment_name: kg_eval
  logger: ['console', 'wandb']
  log_val_generations: 0
  rollout_data_dir: null
  validation_data_dir: null
  nnodes: 1
  n_gpus_per_node: 4
  save_freq: -1
  resume_mode: disable
  resume_from_path: null
  val_before_train: False
  test_freq: -1
  critic_warmup: 0
  default_hdfs_dir: null
  del_local_ckpt_after_load: False
  default_local_dir: evaluation_results
  max_actor_ckpt_to_keep: null
  max_critic_ckpt_to_keep: null
  ray_wait_register_center_timeout: 300
  device: cuda
  mode: kg-search

# configs related to ray initialization
ray_init:
  num_cpus: null
  timeline_json_file: null

# Evaluation-specific parameters
mode: kg-search
n_rollout_eval: 8
k_values: [1, 3, 5, 8]
eval_samples: 64